{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1926230,"sourceType":"datasetVersion","datasetId":1148896}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers torch sentencepiece\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:58:01.094843Z","iopub.execute_input":"2025-06-21T09:58:01.095075Z","iopub.status.idle":"2025-06-21T09:59:27.970077Z","shell.execute_reply.started":"2025-06-21T09:58:01.095057Z","shell.execute_reply":"2025-06-21T09:59:27.969358Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install -U transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:59:27.971889Z","iopub.execute_input":"2025-06-21T09:59:27.972116Z","iopub.status.idle":"2025-06-21T09:59:45.494288Z","shell.execute_reply.started":"2025-06-21T09:59:27.972093Z","shell.execute_reply":"2025-06-21T09:59:45.493347Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nCollecting transformers\n  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.3\n    Uninstalling transformers-4.51.3:\n      Successfully uninstalled transformers-4.51.3\nSuccessfully installed transformers-4.52.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\ndataset = Dataset.from_csv('/kaggle/input/en-fr-translation-dataset/en-fr.csv') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:59:45.495181Z","iopub.execute_input":"2025-06-21T09:59:45.495427Z","iopub.status.idle":"2025-06-21T10:04:21.448262Z","shell.execute_reply.started":"2025-06-21T09:59:45.495401Z","shell.execute_reply":"2025-06-21T10:04:21.447655Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89876bbb2d1f4fd5be90972536b6cb07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading dataset shards:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f440069f9e4474a898e03127a51dd9c"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from datasets import DatasetDict\n\n##splitting dataset in test and train sets\nsplit = dataset.train_test_split(test_size=0.2)\n\ndataset = DatasetDict({\n    'train': split['train'],\n    'validation': split['test']\n})\n\n\ndataset['train'] = dataset['train'].select(range(min(200000, len(dataset['train']))))\ndataset['validation'] = dataset['validation'].select(range(min(100000, len(dataset['validation']))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:04:21.449067Z","iopub.execute_input":"2025-06-21T10:04:21.449495Z","iopub.status.idle":"2025-06-21T10:04:27.986890Z","shell.execute_reply.started":"2025-06-21T10:04:21.449475Z","shell.execute_reply":"2025-06-21T10:04:27.986291Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import MarianTokenizer\n\nmodel_name = 'Helsinki-NLP/opus-mt-en-fr'\ntokenizer = MarianTokenizer.from_pretrained(model_name)\n\ndef preprocess_function(examples):\n    ##Ensuring inputs are string and handling the non values\n    inputs = [str(text) if text is not None else \"\" for text in examples['en']]\n    targets = [str(text) if text is not None else \"\" for text in examples['fr']]\n    \n    ##Tokenizing the inputs\n    model_inputs = tokenizer(\n        inputs, \n        max_length=64, \n        truncation=True, \n        padding='max_length',\n        return_tensors=None\n    )\n    \n    ## Tokenizing the targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets, \n            max_length=64, \n            truncation=True, \n            padding='max_length',\n            return_tensors=None  \n        )\n    \n    ## Replacing padding token with -100 in labels\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n        for label in labels[\"input_ids\"]\n    ]\n    \n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs\n\n## Preprocessing 2\nprint(\"Re-processing dataset -.-.-.\")\ntokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)\n\n## Verifing data format\nprint(\"Sample processed data:\")\nprint(tokenized_datasets['train'][0])\nprint(\"Keys:\", tokenized_datasets['train'].column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:04:27.989657Z","iopub.execute_input":"2025-06-21T10:04:27.989894Z","iopub.status.idle":"2025-06-21T10:08:17.369846Z","shell.execute_reply.started":"2025-06-21T10:04:27.989869Z","shell.execute_reply":"2025-06-21T10:08:17.369265Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f221369b944440c8501822cd9437cdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1909561316b0485983e637c0dd3ecbc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f02ffe43f990405cb7280ef3652f4bee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd56580376aa486a929f6025a20d8ee7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5a6c923ad704334a8369a8b06b694e7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:177: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"name":"stdout","text":"Re-processing dataset -.-.-.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67ec8aeef6464443a1d140b27c6a510e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a027e53a058d4631bbc82e0dacc5614e"}},"metadata":{}},{"name":"stdout","text":"Sample processed data:\n{'input_ids': [1671, 3, 60, 6274, 226, 23740, 6, 9, 748, 32, 5523, 30, 35508, 33342, 442, 18, 12415, 10, 1057, 29879, 3509, 48, 276, 48, 18, 31433, 7068, 10, 19, 4096, 27039, 3, 0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [6929, 3670, 95, 37079, 36, 14, 6, 1078, 13, 26219, 30050, 2651, 31, 19, 252, 22, 3509, 22, 1250, 11, 5, 8, 33107, 2, 11, 36, 16, 14, 7131, 14270, 3894, 11, 8, 19, 5229, 15631, 3, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\nKeys: ['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import MarianMTModel, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nmodel = MarianMTModel.from_pretrained(model_name)\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\n## Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir='./results',\n    per_device_train_batch_size=16,\n    num_train_epochs=1,\n    max_steps=30,  \n    logging_steps=5,  \n    save_strategy='no',\n    eval_strategy='no',  \n    disable_tqdm=False,  \n    fp16=True,\n    dataloader_num_workers=0,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:08:17.370859Z","iopub.execute_input":"2025-06-21T10:08:17.371395Z","iopub.status.idle":"2025-06-21T10:08:51.054243Z","shell.execute_reply.started":"2025-06-21T10:08:17.371369Z","shell.execute_reply":"2025-06-21T10:08:51.053577Z"}},"outputs":[{"name":"stderr","text":"2025-06-21 10:08:22.238986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750500502.809452      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750500502.944167      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e115a3bcb60f455b9c4bbe1412c3c735"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91d73179d9b645879df7cf354cfade8b"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n## Manual training \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.train()\n\n## Small dataloader for testing\nsmall_dataset = tokenized_datasets['train'].select(range(50))\ntrain_dataloader = DataLoader(small_dataset, batch_size=8, collate_fn=data_collator)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n\nprint(\"=== Manual Training Started ===\")\ntotal_steps = 10\n\nwith tqdm(total=total_steps, desc=\"Training\") as pbar:\n    step_count = 0\n    for epoch in range(1):\n        epoch_loss = 0\n        for batch in train_dataloader:\n            if step_count >= total_steps:\n                break\n                \n            ## Moves batch to device\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            ## Forward pass\n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            ## Backward pass\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            ## Update progress\n            step_count += 1\n            epoch_loss += loss.item()\n            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n            pbar.update(1)\n            \n            ## Print every few steps\n            if step_count % 3 == 0:\n                print(f\"Step {step_count}/{total_steps} - Loss: {loss.item():.4f}\")\n\nprint(f\"=== Training Completed - Average Loss: {epoch_loss/step_count:.4f} ===\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:08:51.055017Z","iopub.execute_input":"2025-06-21T10:08:51.055616Z","iopub.status.idle":"2025-06-21T10:08:53.390095Z","shell.execute_reply.started":"2025-06-21T10:08:51.055596Z","shell.execute_reply":"2025-06-21T10:08:53.389566Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/301M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67c2ef9184d54648a28dfcc5ade51f4c"}},"metadata":{}},{"name":"stdout","text":"=== Manual Training Started ===\n","output_type":"stream"},{"name":"stderr","text":"\nTraining:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\nTraining:   0%|          | 0/10 [00:01<?, ?it/s, loss=1.6320]\u001b[A\nTraining:  10%|█         | 1/10 [00:01<00:11,  1.27s/it, loss=1.6320]\u001b[A\nTraining:  10%|█         | 1/10 [00:01<00:11,  1.27s/it, loss=1.8307]\u001b[A\nTraining:  20%|██        | 2/10 [00:01<00:04,  1.63it/s, loss=1.8307]\u001b[A\nTraining:  20%|██        | 2/10 [00:01<00:04,  1.63it/s, loss=2.1278]\u001b[A\nTraining:  30%|███       | 3/10 [00:01<00:02,  2.52it/s, loss=2.1278]\u001b[A\nTraining:  30%|███       | 3/10 [00:01<00:02,  2.52it/s, loss=2.6585]\u001b[A\nTraining:  40%|████      | 4/10 [00:01<00:01,  3.56it/s, loss=2.6585]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Step 3/10 - Loss: 2.1278\n","output_type":"stream"},{"name":"stderr","text":"\nTraining:  40%|████      | 4/10 [00:01<00:01,  3.56it/s, loss=1.7777]\u001b[A\nTraining:  50%|█████     | 5/10 [00:01<00:01,  4.56it/s, loss=1.7777]\u001b[A\nTraining:  50%|█████     | 5/10 [00:01<00:01,  4.56it/s, loss=2.1026]\u001b[A\nTraining:  60%|██████    | 6/10 [00:01<00:00,  5.56it/s, loss=2.1026]\u001b[A\nTraining:  60%|██████    | 6/10 [00:01<00:00,  5.56it/s, loss=0.9897]\u001b[A\nTraining:  70%|███████   | 7/10 [00:01<00:00,  3.50it/s, loss=0.9897]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Step 6/10 - Loss: 2.1026\n=== Training Completed - Average Loss: 1.8741 ===\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\nchencherry = SmoothingFunction()\n\ndef compute_bleu(reference, candidate):\n    return sentence_bleu([reference], candidate, smoothing_function=chencherry.method4)\n\n\ninputs = tokenizer('''I didn't expect the big departure. Close the door without coming back. I know, you don't understand. My life, my choices, and the desire to touch the future.\nNo, I haven't forgotten you, I won't say it.\nBut when I think about it, my chest dances.\nSo, forgive me for all those nights when.\nI prayed to be left alone.''', return_tensors=\"pt\", padding=True).to(model.device)\ntranslated = model.generate(**inputs)\noutput = tokenizer.decode(translated[0], skip_special_tokens=True)\nreference = '''Je ne m' attendais pas à un grand départ.\nFermez la porte sans revenir.\nJe sais, vous ne comprenez pas.\nMa vie, mes choix, et le désir de toucher l'avenir.\nNon, je ne vous ai pas oublié, je ne le dirai pas.\nMais quand j'y pense, ma poitrine danse.\nAlors, pardonne-moi pour tous ces soirs où\nJ'ai prié d'être laissé seul.'''.split()\ncandidate = output.split()\nbleu = compute_bleu(reference, candidate)\nprint(\"BLEU Score:\", bleu)\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:19:06.537403Z","iopub.execute_input":"2025-06-21T10:19:06.537707Z","iopub.status.idle":"2025-06-21T10:19:07.520586Z","shell.execute_reply.started":"2025-06-21T10:19:06.537688Z","shell.execute_reply":"2025-06-21T10:19:07.519955Z"}},"outputs":[{"name":"stdout","text":"BLEU Score: 0.6811146617502504\nJe ne m'attendais pas au grand départ. Fermez la porte sans revenir. Je sais, vous ne comprenez pas. Ma vie, mes choix, et le désir de toucher l'avenir. Non, je ne vous ai pas oublié, je ne le dirai pas. Mais quand j'y pense, mes danses de poitrine. Alors, pardonnez-moi pour toutes ces nuits où. J'ai prié d'être seule.\n","output_type":"stream"}],"execution_count":17}]}